{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/warlords94/personal-coding/blob/main/CCT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4UtMDkNrs7I",
        "outputId": "2101bf70-7aa3-440a-a62e-e1b66a117ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 266 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 296 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 327 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 358 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 501 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 532 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 563 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 593 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 634 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 655 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 665 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 686 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 716 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 737 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 747 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 757 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 768 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 778 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 788 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 808 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 819 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 829 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 839 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 849 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 860 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 890 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 921 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 931 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 952 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 962 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 983 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 993 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.0 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.0 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 10.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 10.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -U -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_rVroU_FnkR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCBQyCzbFsBl"
      },
      "outputs": [],
      "source": [
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqysh9k-UXwG",
        "outputId": "17617873-6865-410f-bf1a-fca1d62e76c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n",
            "170508288/170498071 [==============================] - 4s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhnGCTHLqR3J"
      },
      "outputs": [],
      "source": [
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # This is our tokenizer.\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # Positional embeddings are optional in CCT. Here, we calculate\n",
        "        # the number of sequences and initialize an `Embedding` layer to\n",
        "        # compute the positional embeddings later.\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS8ynMUC_18u"
      },
      "outputs": [],
      "source": [
        "# Referred from: github.com:rwightman/pytorch-image-models.\n",
        "class StochasticDepth(layers.Layer):\n",
        "    def __init__(self, drop_prop, **kwargs):\n",
        "        super(StochasticDepth, self).__init__(**kwargs)\n",
        "        self.drop_prob = drop_prop\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if training:\n",
        "            keep_prob = 1 - self.drop_prob\n",
        "            shape = (tf.shape(x)[0],) + (1,) * (tf.shape(x).shape[0] - 1)\n",
        "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
        "            random_tensor = tf.floor(random_tensor)\n",
        "            return (x / keep_prob) * random_tensor\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySlBmj0BuYYr"
      },
      "outputs": [],
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTrzIqAbqMXO"
      },
      "outputs": [],
      "source": [
        "# Note the rescaling layer. These layers have pre-defined inference behavior.\n",
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(scale=1.0 / 255),\n",
        "        layers.RandomCrop(image_size, image_size),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"data_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vnLVxPbEUos"
      },
      "outputs": [],
      "source": [
        "def create_cct_model(\n",
        "    image_size = image_size,\n",
        "    input_shape = input_shape,\n",
        "    num_heads=num_heads,\n",
        "    projection_dim =projection_dim,\n",
        "    transformer_units = transformer_units,):\n",
        "  inputs = layers.Input(input_shape)\n",
        "  augmented = data_augmentation(inputs)\n",
        "  cct_tokenizer = CCTTokenizer()\n",
        "  encoded_patches = cct_tokenizer(augmented)\n",
        "  if positional_embed:\n",
        "    pos_embed,seq_length = cct_tokenizer.positional_embedding(image_size)\n",
        "    positions = tf.range(start=0,limit=seq_length, delta=1)\n",
        "    positional_embeddings = pos_embed(positions)\n",
        "    encoded_patches += positional_embeddings\n",
        "  dpr = [x for x in np.linspace(0,stochastic_depth_rate,transformer_layers)]\n",
        "  for i in range(transformer_layers):\n",
        "    x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "    attention_output = layers.MultiHeadAttention(num_heads = num_heads , key_dim = projection_dim, dropout =0.1)(x1,x1)\n",
        "    attention_output = StochasticDepth(dpr[i])(attention_output)\n",
        "    x2 = layers.Add()([attention_output,encoded_patches])\n",
        "    x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
        "    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "    x3 = StochasticDepth(dpr[i])(x3)\n",
        "    encoded_patches = layers.Add()([x3, x2])\n",
        "  representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
        "  attention_weights = tf.nn.softmax(layers.Dense(1)(representation),axis =1)\n",
        "  weighted_representation = tf.matmul( attention_weights, representation, transpose_a=True)\n",
        "  weighted_representation = tf.squeeze(weighted_representation,-2)\n",
        "  logits = layers.Dense(num_classes)(weighted_representation)\n",
        "  model = keras.Model(inputs = inputs, outputs =logits)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo_t-oGLUr-K"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "  optimizer = tfa.optimizers.AdamW(learning_rate =0.001,weight_decay=0.0001)\n",
        "  model.compile(\n",
        "      optimizer=optimizer,\n",
        "      loss=keras.losses.CategoricalCrossentropy(\n",
        "          from_logits = True, label_smoothing =0.1\n",
        "      ),\n",
        "      metrics =[\n",
        "          keras.metrics.CategoricalAccuracy(name ='accuracy'),\n",
        "          keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "\n",
        "\n",
        "      ],\n",
        "  )\n",
        "  history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        \n",
        "    )\n",
        "  _,accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "  print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "  return history\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp580qkUEMg7"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(\n",
        "            from_logits=True, label_smoothing=0.1\n",
        "        ),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_split=0.1,\n",
        "        callbacks=[checkpoint_callback],\n",
        "    )\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "cct_model = create_cct_model()\n",
        "history = run_experiment(cct_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwY-_SrXd774",
        "outputId": "3b5a2716-8412-4487-8bd0-03a4f5e67825"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "352/352 [==============================] - 396s 1s/step - loss: 1.9354 - accuracy: 0.3209 - top-5-accuracy: 0.8250 - val_loss: 1.7280 - val_accuracy: 0.4176 - val_top-5-accuracy: 0.9092\n",
            "Epoch 2/30\n",
            "352/352 [==============================] - 388s 1s/step - loss: 1.6261 - accuracy: 0.4744 - top-5-accuracy: 0.9225 - val_loss: 1.5542 - val_accuracy: 0.4982 - val_top-5-accuracy: 0.9348\n",
            "Epoch 3/30\n",
            "352/352 [==============================] - 392s 1s/step - loss: 1.4920 - accuracy: 0.5378 - top-5-accuracy: 0.9439 - val_loss: 1.4180 - val_accuracy: 0.5778 - val_top-5-accuracy: 0.9570\n",
            "Epoch 4/30\n",
            "352/352 [==============================] - 390s 1s/step - loss: 1.3917 - accuracy: 0.5906 - top-5-accuracy: 0.9568 - val_loss: 1.4053 - val_accuracy: 0.5792 - val_top-5-accuracy: 0.9646\n",
            "Epoch 5/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 1.3342 - accuracy: 0.6190 - top-5-accuracy: 0.9622 - val_loss: 1.2562 - val_accuracy: 0.6570 - val_top-5-accuracy: 0.9722\n",
            "Epoch 6/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 1.2761 - accuracy: 0.6490 - top-5-accuracy: 0.9670 - val_loss: 1.2499 - val_accuracy: 0.6602 - val_top-5-accuracy: 0.9726\n",
            "Epoch 7/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 1.2372 - accuracy: 0.6640 - top-5-accuracy: 0.9718 - val_loss: 1.2122 - val_accuracy: 0.6840 - val_top-5-accuracy: 0.9730\n",
            "Epoch 8/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 1.2037 - accuracy: 0.6832 - top-5-accuracy: 0.9744 - val_loss: 1.1844 - val_accuracy: 0.6934 - val_top-5-accuracy: 0.9756\n",
            "Epoch 9/30\n",
            "352/352 [==============================] - 388s 1s/step - loss: 1.1779 - accuracy: 0.6944 - top-5-accuracy: 0.9769 - val_loss: 1.2151 - val_accuracy: 0.6806 - val_top-5-accuracy: 0.9716\n",
            "Epoch 10/30\n",
            "352/352 [==============================] - 393s 1s/step - loss: 1.1579 - accuracy: 0.7058 - top-5-accuracy: 0.9780 - val_loss: 1.1789 - val_accuracy: 0.7050 - val_top-5-accuracy: 0.9752\n",
            "Epoch 11/30\n",
            "352/352 [==============================] - 388s 1s/step - loss: 1.1329 - accuracy: 0.7163 - top-5-accuracy: 0.9791 - val_loss: 1.1465 - val_accuracy: 0.7120 - val_top-5-accuracy: 0.9752\n",
            "Epoch 12/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 1.1110 - accuracy: 0.7274 - top-5-accuracy: 0.9815 - val_loss: 1.1177 - val_accuracy: 0.7256 - val_top-5-accuracy: 0.9794\n",
            "Epoch 13/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 1.1016 - accuracy: 0.7310 - top-5-accuracy: 0.9810 - val_loss: 1.1304 - val_accuracy: 0.7192 - val_top-5-accuracy: 0.9790\n",
            "Epoch 14/30\n",
            "352/352 [==============================] - 383s 1s/step - loss: 1.0832 - accuracy: 0.7398 - top-5-accuracy: 0.9832 - val_loss: 1.1242 - val_accuracy: 0.7262 - val_top-5-accuracy: 0.9778\n",
            "Epoch 15/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 1.0661 - accuracy: 0.7463 - top-5-accuracy: 0.9846 - val_loss: 1.1177 - val_accuracy: 0.7276 - val_top-5-accuracy: 0.9798\n",
            "Epoch 16/30\n",
            "352/352 [==============================] - 383s 1s/step - loss: 1.0556 - accuracy: 0.7523 - top-5-accuracy: 0.9846 - val_loss: 1.0843 - val_accuracy: 0.7388 - val_top-5-accuracy: 0.9796\n",
            "Epoch 17/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 1.0454 - accuracy: 0.7564 - top-5-accuracy: 0.9850 - val_loss: 1.1138 - val_accuracy: 0.7360 - val_top-5-accuracy: 0.9824\n",
            "Epoch 18/30\n",
            "352/352 [==============================] - 384s 1s/step - loss: 1.0277 - accuracy: 0.7653 - top-5-accuracy: 0.9862 - val_loss: 1.0889 - val_accuracy: 0.7410 - val_top-5-accuracy: 0.9850\n",
            "Epoch 19/30\n",
            "352/352 [==============================] - 386s 1s/step - loss: 1.0277 - accuracy: 0.7645 - top-5-accuracy: 0.9866 - val_loss: 1.0766 - val_accuracy: 0.7414 - val_top-5-accuracy: 0.9836\n",
            "Epoch 20/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 1.0106 - accuracy: 0.7728 - top-5-accuracy: 0.9871 - val_loss: 1.0635 - val_accuracy: 0.7512 - val_top-5-accuracy: 0.9822\n",
            "Epoch 21/30\n",
            "352/352 [==============================] - 386s 1s/step - loss: 1.0029 - accuracy: 0.7752 - top-5-accuracy: 0.9868 - val_loss: 1.0393 - val_accuracy: 0.7636 - val_top-5-accuracy: 0.9842\n",
            "Epoch 22/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 0.9892 - accuracy: 0.7830 - top-5-accuracy: 0.9887 - val_loss: 1.0586 - val_accuracy: 0.7518 - val_top-5-accuracy: 0.9858\n",
            "Epoch 23/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 0.9834 - accuracy: 0.7854 - top-5-accuracy: 0.9884 - val_loss: 1.0376 - val_accuracy: 0.7686 - val_top-5-accuracy: 0.9820\n",
            "Epoch 24/30\n",
            "352/352 [==============================] - 384s 1s/step - loss: 0.9730 - accuracy: 0.7904 - top-5-accuracy: 0.9886 - val_loss: 1.0096 - val_accuracy: 0.7766 - val_top-5-accuracy: 0.9852\n",
            "Epoch 25/30\n",
            "352/352 [==============================] - 384s 1s/step - loss: 0.9643 - accuracy: 0.7945 - top-5-accuracy: 0.9894 - val_loss: 1.0171 - val_accuracy: 0.7696 - val_top-5-accuracy: 0.9866\n",
            "Epoch 26/30\n",
            "352/352 [==============================] - 385s 1s/step - loss: 0.9646 - accuracy: 0.7936 - top-5-accuracy: 0.9894 - val_loss: 1.0278 - val_accuracy: 0.7710 - val_top-5-accuracy: 0.9844\n",
            "Epoch 27/30\n",
            "352/352 [==============================] - 387s 1s/step - loss: 0.9567 - accuracy: 0.7958 - top-5-accuracy: 0.9895 - val_loss: 0.9944 - val_accuracy: 0.7892 - val_top-5-accuracy: 0.9850\n",
            "Epoch 28/30\n",
            "335/352 [===========================>..] - ETA: 18s - loss: 0.9459 - accuracy: 0.8025 - top-5-accuracy: 0.9892"
          ]
        }
      ],
      "source": [
        "cct_model = create_cct_model()\n",
        "history = run_experiment(cct_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVmMQZv4d8mS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyqejfNefsIY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CCT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMAFgpvgyZd1GHnSADXhviK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}